{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Q1: Part1\n",
    "#\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "def freqFourLetterWords(url):\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    links = soup.find_all('p')\n",
    "    res = dict()\n",
    "    for link in links:\n",
    "        linkText = link.get_text().strip()\n",
    "        rlist = re.findall('[^a-zA-Z][a-zA-Z]{4}[^a-zA-Z]',linkText.lower())\n",
    "        for e in rlist:\n",
    "            e = e[1:-1]\n",
    "            if e in res:\n",
    "                res[e] = res[e] + 1\n",
    "            else:\n",
    "                res[e] = 1\n",
    "    f = open(\"Q1_Part1.txt\",\"w+\")\n",
    "    sorted_res = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "    for e in sorted_res:\n",
    "        f.write(e[0] + \",\" + str(e[1]) + \"\\n\")\n",
    "    f.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'west': 15,\n",
       " 'john': 10,\n",
       " 'land': 4,\n",
       " 'name': 4,\n",
       " 'were': 9,\n",
       " 'with': 38,\n",
       " 'main': 3,\n",
       " 'more': 16,\n",
       " 'over': 9,\n",
       " 'body': 5,\n",
       " 'well': 9,\n",
       " 'very': 3,\n",
       " 'been': 9,\n",
       " 'take': 3,\n",
       " 'bids': 1,\n",
       " 'what': 1,\n",
       " 'from': 28,\n",
       " 'five': 4,\n",
       " 'that': 11,\n",
       " 'than': 4,\n",
       " 'only': 9,\n",
       " 'work': 3,\n",
       " 'also': 21,\n",
       " 'them': 2,\n",
       " 'good': 1,\n",
       " 'part': 5,\n",
       " 'plan': 1,\n",
       " 'next': 3,\n",
       " 'call': 1,\n",
       " 'hall': 28,\n",
       " 'fire': 2,\n",
       " 'into': 4,\n",
       " 'most': 8,\n",
       " 'time': 2,\n",
       " 'home': 5,\n",
       " 'keep': 2,\n",
       " 'half': 1,\n",
       " 'wars': 1,\n",
       " 'ross': 9,\n",
       " 'such': 3,\n",
       " 'camp': 1,\n",
       " 'site': 1,\n",
       " 'flew': 1,\n",
       " 'some': 3,\n",
       " 'army': 1,\n",
       " 'navy': 1,\n",
       " 'bill': 1,\n",
       " 'year': 4,\n",
       " 'four': 4,\n",
       " 'long': 1,\n",
       " 'late': 2,\n",
       " 'arts': 5,\n",
       " 'seal': 2,\n",
       " 'have': 13,\n",
       " 'high': 3,\n",
       " 'give': 2,\n",
       " 'park': 8,\n",
       " 'aims': 1,\n",
       " 'city': 2,\n",
       " 'near': 2,\n",
       " 'bank': 3,\n",
       " 'sits': 2,\n",
       " 'road': 1,\n",
       " 'loop': 1,\n",
       " 'ride': 1,\n",
       " 'mall': 10,\n",
       " 'quad': 1,\n",
       " 'foot': 2,\n",
       " 'tall': 1,\n",
       " 'bell': 2,\n",
       " 'icon': 1,\n",
       " 'many': 6,\n",
       " 'rotc': 1,\n",
       " 'used': 2,\n",
       " 'said': 1,\n",
       " 'made': 3,\n",
       " 'open': 2,\n",
       " 'walk': 4,\n",
       " 'haas': 2,\n",
       " 'fans': 2,\n",
       " 'east': 1,\n",
       " 'club': 7,\n",
       " 'when': 3,\n",
       " 'cost': 2,\n",
       " 'area': 3,\n",
       " 'fine': 1,\n",
       " 'life': 3,\n",
       " 'this': 7,\n",
       " 'both': 2,\n",
       " 'help': 2,\n",
       " 'gift': 1,\n",
       " 'kind': 1,\n",
       " 'play': 1,\n",
       " 'vice': 3,\n",
       " 'ever': 1,\n",
       " 'cary': 4,\n",
       " 'male': 2,\n",
       " 'lady': 1,\n",
       " 'jane': 1,\n",
       " 'atop': 2,\n",
       " 'hill': 3,\n",
       " 'will': 1,\n",
       " 'join': 1,\n",
       " 'mile': 2,\n",
       " 'rise': 1,\n",
       " 'neil': 3,\n",
       " 'moon': 3,\n",
       " 'last': 3,\n",
       " 'word': 1,\n",
       " 'july': 1,\n",
       " 'unit': 3,\n",
       " 'fort': 1,\n",
       " 'wide': 4,\n",
       " 'best': 6,\n",
       " 'gtap': 1,\n",
       " 'held': 1,\n",
       " 'lack': 1,\n",
       " 'bond': 1,\n",
       " 'leed': 1,\n",
       " 'fall': 3,\n",
       " 'live': 3,\n",
       " 'week': 1,\n",
       " 'each': 5,\n",
       " 'news': 2,\n",
       " 'tied': 4,\n",
       " 'came': 1,\n",
       " 'full': 1,\n",
       " 'rest': 2,\n",
       " 'owen': 1,\n",
       " 'wood': 1,\n",
       " 'twin': 1,\n",
       " 'much': 1,\n",
       " 'rent': 1,\n",
       " 'role': 3,\n",
       " 'kids': 1,\n",
       " 'gold': 5,\n",
       " 'rube': 1,\n",
       " 'pgsg': 2,\n",
       " 'fest': 2,\n",
       " 'prix': 3,\n",
       " 'ease': 1,\n",
       " 'team': 8,\n",
       " 'meet': 1,\n",
       " 'pudm': 2,\n",
       " 'hour': 1,\n",
       " 'bowl': 6,\n",
       " 'kart': 1,\n",
       " 'gala': 1,\n",
       " 'pubs': 1,\n",
       " 'purr': 2,\n",
       " 'core': 1,\n",
       " 'itap': 1,\n",
       " 'hsse': 1,\n",
       " 'item': 1,\n",
       " 'days': 2,\n",
       " 'wbaa': 8,\n",
       " 'wccr': 3,\n",
       " 'wily': 1,\n",
       " 'whhr': 1,\n",
       " 'self': 1,\n",
       " 'back': 2,\n",
       " 'show': 1,\n",
       " 'erik': 1,\n",
       " 'soon': 1,\n",
       " 'gave': 1,\n",
       " 'like': 1,\n",
       " 'pete': 1,\n",
       " 'then': 1,\n",
       " 'song': 1,\n",
       " 'hail': 1,\n",
       " 'fees': 1,\n",
       " 'ncaa': 6,\n",
       " 'golf': 1,\n",
       " 'acha': 1,\n",
       " 'dame': 1,\n",
       " 'anna': 1,\n",
       " 'jeff': 1,\n",
       " 'head': 3,\n",
       " 'nine': 1,\n",
       " 'just': 1,\n",
       " 'farm': 1,\n",
       " 'game': 2,\n",
       " 'link': 2,\n",
       " 'even': 1,\n",
       " 'garb': 1,\n",
       " 'hand': 1,\n",
       " 'they': 2,\n",
       " 'bars': 1,\n",
       " 'matt': 1,\n",
       " 'took': 1,\n",
       " 'gene': 1,\n",
       " 'lead': 2,\n",
       " 'sent': 1,\n",
       " 'ohio': 1,\n",
       " 'hold': 1,\n",
       " 'ichi': 2,\n",
       " 'cold': 1,\n",
       " 'leah': 1,\n",
       " 'past': 1,\n",
       " 'gate': 1,\n",
       " 'food': 2,\n",
       " 'pool': 1,\n",
       " 'nasa': 2,\n",
       " 'crew': 1,\n",
       " 'deng': 1,\n",
       " 'corn': 1,\n",
       " 'nina': 1,\n",
       " 'went': 1,\n",
       " 'unep': 1,\n",
       " 'mark': 1,\n",
       " 'rand': 1,\n",
       " 'lamb': 2,\n",
       " 'span': 1,\n",
       " 'kirk': 1,\n",
       " 'earl': 1,\n",
       " 'bayh': 1,\n",
       " 'cain': 1,\n",
       " 'film': 1,\n",
       " 'leer': 1,\n",
       " 'tech': 1,\n",
       " 'hugo': 1,\n",
       " 'rick': 1,\n",
       " 'paul': 1,\n",
       " 'herm': 1,\n",
       " 'brad': 1,\n",
       " 'drew': 1,\n",
       " 'hank': 1,\n",
       " 'ryan': 1,\n",
       " 'acts': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqFourLetterWords(\"https://en.wikipedia.org/wiki/Purdue_University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Q1: Part2\n",
    "#\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "def freqFourLetterWordsNoStopWords(url):\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    links = soup.find_all('p')\n",
    "    f = open(\"stop_words.txt\",\"r+\").readlines()\n",
    "    stop_words = set()\n",
    "    for word in f:\n",
    "        stop_words.add(word[0:-1])\n",
    "    res = dict()\n",
    "    for link in links:\n",
    "        linkText = link.get_text().strip()\n",
    "        rlist = re.findall('[^a-zA-Z][a-zA-Z]{4}[^a-zA-Z]',linkText.lower())\n",
    "        for e in rlist:\n",
    "            e = e[1:-1]\n",
    "            if e in stop_words:\n",
    "                continue\n",
    "            if e in res:\n",
    "                res[e] = res[e] + 1\n",
    "            else:\n",
    "                res[e] = 1\n",
    "    f = open(\"Q1_Part2.txt\",\"w+\")\n",
    "    sorted_res = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "    for e in sorted_res:\n",
    "        f.write(e[0] + \",\" + str(e[1]) + \"\\n\")\n",
    "    f.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 81,\n",
       " 'gray': 2,\n",
       " 'term': 9,\n",
       " 'past': 1,\n",
       " 'used': 5,\n",
       " 'naur': 3,\n",
       " 'wide': 1,\n",
       " 'ifcs': 1,\n",
       " 'kobe': 1,\n",
       " 'time': 1,\n",
       " 'jeff': 1,\n",
       " 'gave': 1,\n",
       " 'work': 2,\n",
       " 'plan': 1,\n",
       " 'tool': 1,\n",
       " 'icsu': 1,\n",
       " 'long': 1,\n",
       " 'jobs': 2,\n",
       " 'role': 1,\n",
       " 'ieee': 2,\n",
       " 'ecda': 2,\n",
       " 'paid': 1,\n",
       " 'free': 1,\n",
       " 'gfkl': 1,\n",
       " 'nate': 1,\n",
       " 'said': 1,\n",
       " 'hand': 1,\n",
       " 'wall': 1,\n",
       " 'dawn': 1,\n",
       " 'like': 3,\n",
       " 'dhar': 2,\n",
       " 'uses': 1,\n",
       " 'goal': 1,\n",
       " 'lieu': 1,\n",
       " 'does': 1,\n",
       " 'size': 1,\n",
       " 'form': 1,\n",
       " 'open': 1,\n",
       " 'suit': 1}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqFourLetterWordsNoStopWords(\"https://en.wikipedia.org/wiki/Data_science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Q2\n",
    "#\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "def linkTexts(url = 'https://en.wikipedia.org/wiki/Data_science'):\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    links = soup.find_all('a')\n",
    "    res = dict()\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href is not None:\n",
    "            if 'http' in href:\n",
    "                linkText = link.get_text().strip()\n",
    "                if href in res:\n",
    "                    res[href].add(linkText)\n",
    "                else:\n",
    "                    res[href] = set()\n",
    "                    res[href].add(linkText)\n",
    "    f = open(\"Q2.txt\",\"w+\")\n",
    "    for e in res:\n",
    "        f.write(e + \"\\n\")\n",
    "        for s in res[e]:\n",
    "            f.write(\"\\t\" + s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkTexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Q3 : part1\n",
    "#\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "\n",
    "def Q3_Part1(url_txt = 'urls.txt'):\n",
    "    f = open(url_txt,\"r+\")\n",
    "    urls = []\n",
    "    for line in f:\n",
    "        urls.append(line[:-1])\n",
    "    unique = list()\n",
    "    words = list()\n",
    "    tf = {'statistics':[], 'analytics':[], 'data':[], 'science':[]}\n",
    "    idf = {'statistics':0, 'analytics':0, 'data':0, 'science':0}\n",
    "    tf_idf = {'statistics':[], 'analytics':[], 'data':[], 'science':[]}\n",
    "    \n",
    "    for url in urls:\n",
    "    ###\n",
    "        r = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(r)\n",
    "        paras = soup.find_all('p')\n",
    "        uniq = set()\n",
    "        token = list()\n",
    "        a_words = {'statistics':0, 'analytics':0, 'data':0, 'science':0}\n",
    "        for para in paras:\n",
    "            text = para.get_text().strip().lower()\n",
    "            text = re.sub(\"\\W+\",\" \",text)\n",
    "            text = text.split()\n",
    "            for word in text:\n",
    "                uniq.add(word)\n",
    "                token.append(word)\n",
    "                if word in a_words:\n",
    "                    a_words[word] = a_words[word] + 1\n",
    "        unique.append(len(uniq))\n",
    "        words.append(len(token))\n",
    "        for a in a_words:\n",
    "            freq = a_words[a]/len(token)\n",
    "            tf[a].append(freq)\n",
    "            if freq > 0 :\n",
    "                idf[a] = idf[a] + 1\n",
    "        \n",
    "    \n",
    "    ###\n",
    "    for a in idf:\n",
    "        idf[a] = math.log((len(urls)/idf[a]))\n",
    "    \n",
    "    for a in tf:\n",
    "        for freq in tf[a]:\n",
    "            tf_idf[a].append(freq*idf[a])\n",
    "    f = open('Q3_Part1.txt','w+')\n",
    "    f.write('unique: ' + str(unique) + '\\n')\n",
    "    f.write('words: ' + str(words) + '\\n')\n",
    "    for a in tf:\n",
    "        f.write('tf ' + a + ': ' + str(tf[a]) + '\\n')\n",
    "    for a in idf:\n",
    "        f.write('idf ' + a + ': ' + str(idf[a]) + '\\n')\n",
    "    for a in tf_idf:\n",
    "        f.write('tf-idf ' + a + ': ' + str(tf_idf[a]) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_Part1(\"urls.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Q3 : part2\n",
    "#\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "\n",
    "def Q3_Part2(url_txt = 'urls.txt'):\n",
    "    f = open(url_txt,\"r+\")\n",
    "    urls = []\n",
    "    for line in f:\n",
    "        urls.append(line[:-1])\n",
    "    f = open(\"stop_words.txt\",\"r+\").readlines()\n",
    "    stop_words = set()\n",
    "    for word in f:\n",
    "        stop_words.add(word[0:-1])\n",
    "    unique = list()\n",
    "    words = list()\n",
    "    tf = {'statistics':[], 'analytics':[], 'data':[], 'science':[]}\n",
    "    idf = {'statistics':0, 'analytics':0, 'data':0, 'science':0}\n",
    "    tf_idf = {'statistics':[], 'analytics':[], 'data':[], 'science':[]}\n",
    "    \n",
    "    for url in urls:\n",
    "        r = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(r)\n",
    "        paras = soup.find_all('p')\n",
    "        uniq = set()\n",
    "        token = list()\n",
    "        a_words = {'statistics':0, 'analytics':0, 'data':0, 'science':0}\n",
    "        for para in paras:\n",
    "            text = para.get_text().strip().lower()\n",
    "            text = re.sub(\"\\W+\",\" \",text)\n",
    "            text = text.split()\n",
    "            for word in text:\n",
    "                if word not in stop_words:\n",
    "                    uniq.add(word)\n",
    "                    token.append(word)\n",
    "                    if word in a_words:\n",
    "                        a_words[word] = a_words[word] + 1\n",
    "        unique.append(len(uniq))\n",
    "        words.append(len(token))\n",
    "        for a in a_words:\n",
    "            freq = a_words[a]/len(token)\n",
    "            tf[a].append(freq)\n",
    "            if freq > 0 :\n",
    "                idf[a] = idf[a] + 1\n",
    "\n",
    "    for a in idf:\n",
    "        idf[a] = math.log((len(urls)/idf[a]))\n",
    "    \n",
    "    for a in tf:\n",
    "        for freq in tf[a]:\n",
    "            tf_idf[a].append(freq*idf[a])\n",
    "    f = open('Q3_Part2.txt','w+')\n",
    "    f.write('unique: ' + str(unique) + '\\n')\n",
    "    f.write('words: ' + str(words) + '\\n')\n",
    "    for a in tf:\n",
    "        f.write('tf ' + a + ': ' + str(tf[a]) + '\\n')\n",
    "    for a in idf:\n",
    "        f.write('idf ' + a + ': ' + str(idf[a]) + '\\n')\n",
    "    for a in tf_idf:\n",
    "        f.write('tf-idf ' + a + ': ' + str(tf_idf[a]) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_Part2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
