{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1\n",
    "\n",
    "Along with any result/output files (e.g. Q1_Part1.txt, see the questions below), please submit all code as a single Jupyter notebook (a .ipynb file). Any paragraph/descriptive/yes-no answers that you want to be considered for grading must be included as separate markdown cells in the Jupyter notebook (rather than as comments in the code), and these cells should accompany the associated questions: graders cannot be expected to hunt through your submission to find answers to individual questions. _Submit all solution files to Blackboard before the deadline: __11:59pm on Wed, Feb 5__._\n",
    "\n",
    "For this homework you can __ONLY__ use the python libraries math and numpy (for mathematical functions like log), urllib and BeautifulSoup (to read and parse web pages) and re (for regular expressions). Please post any questions you have on piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment you will retrieve and parse webpages using BeautifulSoup.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Write a function to parse the text enclosed in paragraph tags of a webpage, finding all __four letter words__, and counting how many times each word appears.\n",
    "\n",
    "The function should:\n",
    "* Take a URL to the webpage as a parameter.\n",
    "* Return a dictionary of words and their corresponding counts\n",
    "\n",
    "Apply the function with input \"<code>https://en.wikipedia.org/wiki/Data_science</code>\", storing the result in a file, \"Q1_Part1.txt\".\n",
    "Each line of the Q1_Part1.txt should have the form: \"word,frequency\".\n",
    "The words should be sorted in decreasing order of frequency, with the most frequent word appearing at the top and the least frequent word at the end.\n",
    "\n",
    "Example: <br/>\n",
    "<code>\n",
    "math,10\n",
    "cell,8\n",
    "code,6\n",
    "rise,3\n",
    "open,2\n",
    "auto,1\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqFourLetterWords(url):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"also\", and similar words. The file \"stop_work.txt\" contains a list of stop words (this file should be in the same directory as this notebook).\n",
    "\n",
    "Repeat Part 1 for the function freqFourLetterWordsNoStopWords, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqFourLetterWordsNoStopWords(url):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (8pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now write a function that takes a url as input, and finds and counts all outgoing links to other webpages. Again, apply the function with input \"<code>https://en.wikipedia.org/wiki/Data_science</code>\" and write the output to a file named \"Q2.txt\", with each outgoing url on a new line. Following the url, denote each UNIQUE string of text of used by the link within the document (indented with a tab, '\\t').\n",
    "\n",
    "For example, a very simple page with 4 links (http://google.com and http://purdue.edu each twice) may look like this, if both Purdue University links use the same string as text:\n",
    "\n",
    "http://google.com <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Google <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;That one search engine <br/>\n",
    "http://purdue.edu <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Purdue University\n",
    "\n",
    "Denote links with no text by using the empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkTexts(url):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (8pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "__Retrieve and parse multiple web pages__. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "1. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation: use regular expressions involving \\W to replace all sequences of non alpha-numeric characters with a whitespace.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    1. Find _Term Frequency (tf)_. \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    2. Find _Inverse Document Frequency (idf)_.\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    3. Find _tf-idf_. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below (include the labels on each line):\n",
    "\n",
    "unique: [641, 728, 281]  <br/>\n",
    "words: [1606, 2040, 584]  <br/>\n",
    "tf statistics: [0.008717310087173101, 0.0024509803921568627, 0.0]  <br/>\n",
    "tf analytics: [0.0037359900373599006, 0.0029411764705882353, 0.0]  <br/>\n",
    "tf data: [0.05666251556662515, 0.052941176470588235, 0.0]  <br/>\n",
    "tf science: [0.040473225404732256, 0.010784313725490196, 0.0273972602739726]  <br/>\n",
    "idf statistics: 0.4054651081081644  <br/>\n",
    "idf analytics: 0.4054651081081644  <br/>\n",
    "idf data: 0.4054651081081644  <br/>\n",
    "idf science: 0.0  <br/>\n",
    "tf-idf statistics: [0.0035345650769080333, 0.0009937870296768735, 0.0]  <br/>\n",
    "tf-idf analytics: [0.0015148136043891573, 0.0011925444356122481, 0.0]  <br/>\n",
    "tf-idf data: [0.022974672999902215, 0.021465799841020466, 0.0]  <br/>\n",
    "tf-idf science: [0.0, 0.0, 0.0] <br/>\n",
    "\n",
    "\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (8pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sample output for the file \"urls.txt\"\n",
    "\n",
    "unique: [551, 607, 232] <br/>\n",
    "words: [989, 1192, 378] <br/>\n",
    "tf statistics: [0.014155712841253791, 0.0041946308724832215, 0.0] <br/>\n",
    "tf analytics: [0.006066734074823054, 0.0050335570469798654, 0.0] <br/>\n",
    "tf data: [0.09201213346814964, 0.09060402684563758, 0.0] <br/>\n",
    "tf science: [0.06572295247724974, 0.018456375838926176, 0.042328042328042326] <br/>\n",
    "idf statistics: 0.4054651081081644 <br/>\n",
    "idf analytics: 0.4054651081081644 <br/>\n",
    "idf data: 0.4054651081081644 <br/>\n",
    "idf science: 0.0 <br/>\n",
    "tf-idf statistics: [0.005739647637527099, 0.0017007764601852534, 0.0] <br/>\n",
    "tf-idf analytics: [0.0024598489875116143, 0.0020409317522223037, 0.0] <br/>\n",
    "tf-idf data: [0.03730770964392614, 0.03673677154000147, 0.0] <br/>\n",
    "tf-idf science: [0.0, 0.0, 0.0] <br/>\n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
